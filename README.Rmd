---
output: github_document
editor_options: 
  markdown: 
    wrap: 72
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = FALSE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
options(tibble.print_min = 5)
options(tibble.print_max = 10)
set.seed(949958487)
```

# fastplyr

<!-- badges: start -->

[![R-CMD-check](https://github.com/NicChr/fastplyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/NicChr/fastplyr/actions/workflows/R-CMD-check.yaml)
[![CRAN
status](https://www.r-pkg.org/badges/version/fastplyr)](https://CRAN.R-project.org/package=fastplyr)
[![Codecov test
coverage](https://codecov.io/gh/NicChr/fastplyr/graph/badge.svg)](https://app.codecov.io/gh/NicChr/fastplyr)

<!-- badges: end -->

fastplyr aims to provide a [tidyverse](https://www.tidyverse.org/learn)
frontend using a
[collapse](https://sebkrantz.github.io/collapse/articles/collapse_intro.html)
backend. This means from a user's point of view the functions behave
like the tidyverse equivalents and thus require little to no changes to
existing code to convert.

fastplyr is designed to handle operations that involve larger numbers of
groups and generally larger data.

## Installation

You can install the development version of fastplyr from
[GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("NicChr/fastplyr")
```

Load packages

```{r example}
library(tidyverse)
library(fastplyr)
library(nycflights13)
library(bench)
```

While the syntax and user-interface of fastplyr aligns very closely with
dplyr most of the time, there can be a few key differences.

## Differences between fastplyr and dplyr

+---------------+----------------------+-----------------------------+
|               | ### dplyr            | ### fastplyr                |
+===============+======================+=============================+
| `.by`         | Groups are sorted by | Groups are always sorted by |
|               | order of first       | default, even when using    |
|               | appearance always    | `.by`. One can use the      |
|               | when using `.by`     | other by setting            |
|               |                      | `.order = FALSE`            |
+---------------+----------------------+-----------------------------+
| Many groups   | Generally slow for   | Designed to be fast for     |
|               | data with many       | data with many groups.      |
|               | groups.              |                             |
+---------------+----------------------+-----------------------------+
| Handling of   | Executes expressions | Some expressions are        |
| `...`         | in a way that latter | executed independently to   |
| expressions   | expressions depend   | each other                  |
|               | on prior ones        |                             |
+---------------+----------------------+-----------------------------+
| Optimisations | Expressions are run  | Many functions are          |
|               | by-group with        | optimised to either ignore  |
|               | minimal overhead,    | groups or use faster        |
|               | slow for many groups | methods                     |
+---------------+----------------------+-----------------------------+
| Duplicate     | No dedicated         | Dedicated function          |
| rows          | function for this,   | `f_duplicates` can do this  |
|               | solution using       | very fast and with fine     |
|               | `group_ by           | control.                    |
|               |  |> filter(n() > 1)` |                             |
|               | are generally slow   |                             |
|               | for larger data.     |                             |
+---------------+----------------------+-----------------------------+
| Row slicing   | `slice()` supports   | Data-masked expressions not |
|               | data-masked          | supported in `f_slice_`     |
|               | expressions supplied | functions. Use `f_filter()` |
|               | to `...`             | for this behaviour.         |
+---------------+----------------------+-----------------------------+
| Memory usage  | High memory usage    | Lower usage compared to     |
|               |                      | dplyr                       |
+---------------+----------------------+-----------------------------+
| joins         | Accepts different    | Accepts only equality joins |
|               | types of joins, e.g. | of the form `x == y`        |
|               | rolling and equality |                             |
|               | joins.               |                             |
+---------------+----------------------+-----------------------------+
| rowwise       | `rowwise_df`         | `rowwise_df` not accepted,  |
|               | accepted and         | must use `f_rowwise_df`     |
|               | everything           | which creates a             |
|               | sub-setted implictly | `grouped_df` with a row ID  |
|               | using `[[`           | col. Implicit `[[`          |
|               |                      | subsetting does not occur.  |
+---------------+----------------------+-----------------------------+
| Matrices in   | Fully supported      | Not supported               |
| data frames   |                      |                             |
+---------------+----------------------+-----------------------------+
| Grouped data  | N/A                  | `f_group_by` produces a     |
| frames        |                      | `grouped_df` with some      |
|               |                      | additional metadata to      |
|               |                      | assist with making later    |
|               |                      | operations faster           |
+---------------+----------------------+-----------------------------+

## dplyr alternatives

All tidyverse alternative functions are prefixed with 'f\_'. For
example, `dplyr::distinct` becomes `fastplyr::f_distinct`.

### distinct

```{r}
flights |> 
  f_distinct(origin, dest)
```

`f_distinct` has an additional `.order` argument which is much faster
than sorting afterwards.

```{r}
mark(
  fastplyr_distinct_sort = flights |> 
  f_distinct(across(where(is.numeric)), .order = TRUE),
  dplyr_distinct_sort = flights |> 
    distinct(across(where(is.numeric))) |> 
    arrange_all()
)
```

### group_by

`f_group_by` operates very similarly with an additional feature that
allows you to specify whether group data should be ordered or not. This
ultimately controls if the groups end up sorted in expressions like
`count` and `summarise`, but also in this case `f_count` and
`f_summarise`.

```{r}
# Like dplyr
flights |> 
  f_group_by(month) |> 
  f_count()

# Group data is sorted by order-of-first appearance
flights |> 
  f_group_by(month, .order = FALSE) |> 
  f_count()
```

Just a reminder that all fastplyr functions are interchangeable with
dplyr ones both ways

```{r}

### With dplyr::count

flights |> 
  f_group_by(month) |> 
  count()
```

```{r}

### With dplyr::group_by

flights |> 
  group_by(month) |> 
  f_count()
```

### summarise

`f_summarise` behaves like dplyr's `summarise` except for two things:

-   It evaluates expressions independently
-   There are optimisations for common statistical functions which are
    very fast for many groups

```{r}
grouped_flights <- flights |> 
  group_by(across(where(is.character)))

grouped_flights |> 
  f_summarise(
    n = n(), mean_dep_delay = mean(dep_delay)
  )
```

And a benchmark

```{r}
mark(
  fastplyr_summarise = grouped_flights |> 
  f_summarise(
    n = n(), mean_dep_delay = mean(dep_delay)
  ),
  dplyr_summarise = grouped_flights |> 
  summarise(
    n = n(), mean_dep_delay = mean(dep_delay, na.rm = TRUE),
    .groups = "drop"
  )
)
```

### Joins

Joins work much the same way as in dplyr.

```{r}
left <- flights |> 
  f_select(origin, dest, time_hour)
hours <- sample(unique(left$time_hour), 5000)
right <- as.data.frame(unclass(as.POSIXlt(hours)))
right$time_hour <- hours

# Left join

left |> 
  f_left_join(right)

# inner join

left |> 
  f_inner_join(right)

# Anti join

left |> 
  f_anti_join(right)

# Semi join

left |> 
  f_semi_join(right)

# full join

left |> 
  f_full_join(right)
```

And a benchmark comparing fastplyr and dplyr joins

```{r}
mark(
  fastplyr_left_join = f_left_join(left, right, by = "time_hour"),
  dplyr_left_join = left_join(left, right, by = "time_hour")
)
```

```{r}
mark(
  fastplyr_inner_join = f_inner_join(left, right, by = "time_hour"),
  dplyr_inner_join = inner_join(left, right, by = "time_hour")
)
```

```{r}
mark(
  fastplyr_anti_join = f_anti_join(left, right, by = "time_hour"),
  dplyr_anti_join = anti_join(left, right, by = "time_hour")
)
```

```{r}
mark(
  fastplyr_semi_join = f_semi_join(left, right, by = "time_hour"),
  dplyr_semi_join = semi_join(left, right, by = "time_hour")
)
```

```{r}
mark(
  fastplyr_full_join = f_full_join(left, right, by = "time_hour"),
  dplyr_full_join = full_join(left, right, by = "time_hour")
)
```

### slice

`f_slice` and other `f_slice_` functions are very fast for many groups.

```{r}
grouped_flights |> 
  f_slice(1)

grouped_flights |>
  f_slice_head(3)
```

A quick benchmark to prove the point

```{r}
mark(
    fastplyr_slice = grouped_flights |> 
    f_slice_head(n = 3),
    dplyr_slice = grouped_flights |>
        slice_head(n = 3)
)
```

### Group metadata

Group metadata helpers like `cur_group_id()` get optimised in `f_mutate`

```{r}

## Unique ID for each group

mark(
  dplyr = grouped_flights |> 
  f_mutate(group_id = cur_group_id(), .keep = "none"),
  fastplyr = grouped_flights |> 
  mutate(group_id = cur_group_id(), .keep = "none")
)

```

### expand

Based closely on `tidyr::expand`, `f_expand()` can cross joins multiple
vectors and data frames.

```{r}
mark(
    fastplyr_expand = flights |> 
        f_group_by(origin, tailnum) |> 
        f_expand(month = 1:12),
    tidyr_expand = flights |> 
        group_by(origin, tailnum) |> 
        expand(month = 1:12),
    check = FALSE
)


# Using `.cols` in `f_expand()` is very fast!
mark(
    fastplyr_expand = flights |> 
        f_group_by(origin, dest) |> 
        f_expand(.cols = c("year", "month", "day")),
    tidyr_expand = flights |> 
        group_by(origin, dest) |> 
        expand(year, month, day),
    check = FALSE
)
```

### duplicate rows

Finding duplicate rows is a very common dataset operation and there is a
dedicated function `f_duplicates()` to do exactly this.

```{r}
flights |> 
  f_duplicates(time_hour)
```

Benchmark against a common dplyr strategy for finding duplicates

```{r}
mark(
 fastplyr_duplicates = flights |> 
   f_duplicates(time_hour, .both_ways = TRUE, .add_count = TRUE, .keep_all = TRUE),
 dplyr_duplicates = flights |> 
   add_count(time_hour) |> 
   filter(n > 1)
)
```

### filter

In the worst-case scenarios, `f_filter()` is about the same speed as
`filter()` and in the best-case is much faster and more efficient. This
is especially true for large data where small subsets of the data are
returned.

```{r}
full <- new_tbl(x = rnorm(5e07))

# A worst case scenario

mark(
  fastplyr_filter = full |> 
    f_filter(abs(x) > 0),
  dplyr_filter = full |> 
    filter(abs(x) > 0)
)

# Best case scenario - filter results in small subset

mark(
  fastplyr_filter = full |> 
    f_filter(x > 4),
  dplyr_filter = full |> 
    filter(x > 4)
)
```

### bind rows and cols

Binding columns is particular much faster but binding rows is also
sufficiently faster

```{r}
mark(
  fastplyr_bind_cols = f_bind_cols(grouped_flights, grouped_flights),
  dplyr_bind_cols = suppressMessages(
    bind_cols(grouped_flights, grouped_flights)
    ),
  check = FALSE
)

mark(
  fastplyr_bind_rows = f_bind_rows(grouped_flights, grouped_flights),
  dplyr_bind_rows = bind_rows(grouped_flights, grouped_flights)
)
```

### Quantiles

A typical tidy approach might use a mixture of `reframe()` and
`enframe()` which is a perfectly tidy and neat solution

```{r}
probs <- seq(0, 1, 0.25)

mtcars <- as_tbl(mtcars)

mtcars |> 
 group_by(cyl) |> 
 reframe(enframe(quantile(mpg, probs), "quantile", "mpg"))
```

fastplyr though has a dedicated function for quantile calculation,
`tidy_quantiles()` which requires less code to type

```{r}

# Wide
mtcars |> 
  tidy_quantiles(mpg, .by = cyl, pivot = "wide")

# Long
mtcars |> 
  tidy_quantiles(mpg, .by = cyl, pivot = "long")
```

Not only can you choose how to pivot as shown above, you can also
calculate quantiles for multiple variables.

```{r}
multiple_quantiles <- mtcars |> 
  tidy_quantiles(across(where(is.numeric)), pivot = "long")
multiple_quantiles

# Quantile names is a convenient factor
multiple_quantiles$.quantile
```

### Quantile benchmark for many groups

`tidy_quantiles()` of course is fast when many groups are involved.

```{r}
mark(
  fastplyr_quantiles = flights |> 
  tidy_quantiles(dep_delay, pivot = "long",
                 .by = c(year, month, day, origin)),
  dplyr_quantiles = flights |> 
     group_by(year, month, day, origin) |> 
    reframe(enframe(quantile(dep_delay, seq(0, 1, 0.25), na.rm = TRUE))),
  check = FALSE
)
```

### Details on internally optimised functions

fastplyr categorises all expressions into one of 3 categories

1.  Standard expressions
2.  Group-unaware expressions
3.  Group-aware optimisable expressions

The first category are normal expressions which simply don't belong to
the other 2 categories and are evaluated normally.

The second category consists of group-unaware expressions. These can be
be evaluated once on the entire data instead of by-group. For example
the plus function `+` is group-unaware.

The third category consists of functions that are group-aware but can be
optimised, such as most of the common statistical functions like `sum`,
`mean`, etc.

### Group-unaware functions

Some common base R functions can be thought of as group-unaware in the
sense that they return the same results regardless of if they are called
in a grouped context.

fastplyr evaluates these functions once as if there are no groups.

Current list of functions marked as group-unaware

```{r}
fns <- get_group_unaware_fns()

names(fns)

# base::round for example
fns$round
```

An expression is marked as group-unaware if and only if all calls in the
call-tree are group-unaware.

```{r}

# Group-unaware fn names
fn_names <- names(fns)

expr <- quote(x - y)
rlang::is_call(expr, "-")

expr <- quote(x - y + z)

# Top-level expr is a group-unaware call
rlang::is_call(expr, "+")

# `-` expression nested inside is also group-unaware
expr |> 
  as.list() |> 
  pluck(2) |> 
  print() |> 
  rlang::is_call(fn_names)

# Definitely group-aware as `sum()` depends on the group-context
expr <- quote(sum(x - y))
rlang::is_call(expr, fn_names)
```

This allows us to write out more complex expressions and evaluate them
very efficiently

```{r}
mark(
    fastplyr = grouped_flights |> 
        f_mutate(x = round(abs(arr_time - dep_time)), .keep = "none"), 
    dplyr = grouped_flights |> 
        mutate(x = round(abs(arr_time - dep_time)), .keep = "none")
)
```

### Group-aware optimised functions

fastplyr also optimises many common statistical functions like `sum`,
`mean` for use on large grouped data frames.

A list of currently optimised group-aware functions can be viewed in
`f_summarise.Rd` or by running `?f_summarise` in Rstudio.

```{r}
res <- grouped_flights |> 
  f_summarise(across(where(is.numeric), mean)) |> 
  mark()
res$result;res
```

Other group-aware functions that fastplyr optimises include dplyr group
metadata functions like `n()`, `row_number()`, `cur_group_id()`, etc.

```{r}
grouped_flights |> 
  f_mutate(
    n = n(),
    row_id = row_number(),
    group_id = cur_group_id(),
    group_locs = cur_group_rows(),
    .keep = "none"
  )
```

Lags and leads are also optimised by-group

```{r}
flights |> 
  f_mutate(
    time_hour,
    lag = lag(time_hour),
    lead = lead(time_hour),
    .by = origin,
    .keep = "none"
  )
```

The caveat about this approach is that the usual behaviour of
expressions being able to reference the results of previous expressions
is lost when combining standard and non-standard expressions.

Here is an example of this

```{r,error=TRUE}
iris <- as_tbl(iris)

iris |> 
    f_reframe(
        x = Sepal.Length + 1, # Optimised
        y = mean(sum(x)),  # Not currently optimised
        .by = Species
    )
```

To get around this, simply call `f_reframe()` again or `f_mutate()`

```{r}
iris |> 
  f_reframe(x = Sepal.Length + 1, .by = Species) |> 
  f_mutate(y = mean(sum(x)), .by = Species)
```

## tidytable vs fastplyr

Let's run some more benchmarks for fun, this time including tidytable
which fastplyr is very similar to as it also uses a tidy frontend but a
data.table backend

### 10 million rows

```{r}
n_rows <- 10^7
n_groups <- 10^6

tbl <- new_tbl(x = rnorm(n_rows))
tbl <- tbl |> 
    mutate(y = as.character(round(x, 6)),
           g = sample.int(n_groups, n_rows, TRUE))
tbl
```

### slice benchmark

For this we will be using the `.by` argument from each package. Because
fastplyr still sorts the groups by default here we will set an internal
option to use the alternative grouping algorithm that sorts groups by
order of first appearance. This will likely be revisited at some point.

To read about the differences, see `?collapse::GRP`.

```{r}
library(tidytable)

tidy_tbl <- as_tidytable(tbl)

# Setting an internal option to set all grouping to use the non-sorted type
options(.fastplyr.order.groups = FALSE)
tidytable::setDTthreads(1) # Single-threaded for fair comparison

mark(
  fastplyr_slice = tbl |> 
  f_slice(3:5, .by = g),
  tidytable_slice = tidy_tbl |> 
    slice(3:5, .by = g),
  check = FALSE,
  min_iterations = 3
)
```

### slice_head & slice_tail

```{r}
mark(
  fastplyr_slice_head = tbl |> 
  f_slice_head(n = 3, .by = g),
  tidytable_slice_head = tidy_tbl |> 
    slice_head(n = 3, .by = g),
  fastplyr_slice_tail = tbl |> 
  f_slice_tail(n = 3, .by = g),
  tidytable_slice_tail = tidy_tbl |> 
    slice_tail(n = 3, .by = g),
  check = FALSE,
  min_iterations = 3
)
```

### summarise benchmark

Here we'll calculate the mean of x by each group of g

Both tidytable and fastplyr have optimisations for `mean()` when it
involves groups. tidytable internally uses data.table's 'gforce' mean
function. This is basically a dedicated C function to calculate means
for many groups.

```{r}
mark(
  fastplyr_sumarise = tbl |> 
  f_summarise(mean = mean(x), .by = g),
  tidytable_sumarise = tidy_tbl |> 
  summarise(mean = mean(x), .by = g, .sort = FALSE),
  check = FALSE,
  min_iterations = 3
)
```

Benchmarking more statistical functions

```{r}
mark(
  fastplyr_sumarise2 = tbl |> 
  f_summarise(n = dplyr::n(), mean = mean(x), min = min(x), max = max(x), .by = g),
  tidytable_sumarise2 = tidy_tbl |> 
  summarise(n = n(), mean = mean(x), min = min(x), max = max(x), 
            .by = g, .sort = FALSE),
  check = FALSE,
  min_iterations = 3
)
```

### count benchmark

```{r}
mark(
  fastplyr_count = tbl |> 
    f_count(y, g),
  tidytable_count = tidy_tbl |> 
    count(y, g),
  check = FALSE,
  min_iterations = 3
)
```

It's clear both fastplyr and tidytable are fast and each have their
strengths and weaknesses.
